# Airflow-Job_Pipeline

![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/apache%20air.jpg)


## Objetivo del Proyecto
El objetivo es crear un pipeline que cargue los datos autom谩ticamente una vez al d铆a. 
Esto permitir谩 mantener los datos actualizados y disponibles para an谩lisis continuos sin intervenci贸n manual.
Adem谩s, los datos finales ser谩n cargados en AWS S3, asegurando que est茅n accesibles y almacenados de manera segura.


## Sobre los datos con los que vamos a trabajar

Descripci贸n de los datos y su utilidad para el estudio
Este conjunto de datos contiene todas las solicitudes de trabajo presentadas a trav茅s de las oficinas de los distritos,
mediante eFiling o a trav茅s de HUB, que tienen una "Fecha de 煤ltima acci贸n" desde el 1 de enero de 2000. Este conjunto de datos no incluye trabajos presentados a trav茅s de DOB NOW.
[El enlace de la p谩gina web que proporciona los datos.](https://data.cityofnewyork.us/Housing-Development/DOB-Job-Application-Filings/ic3t-wcy2/about_data)

## Utilidad del estudio de estos datos

**Tendencias hist贸ricas:** Analizar las solicitudes de trabajo a lo largo del tiempo puede 
mostrar c贸mo han cambiado las tendencias en la industria de la construcci贸n.

**Diferencias regionales:** Identificar c贸mo se distribuyen las solicitudes en diferentes distritos y oficinas.

**Tipos de proyectos:** Comparar las solicitudes por diferentes tipos de proyectos para identificar patrones predominantes.

**Velocidad de procesamiento de solicitudes:** Evaluar cu谩nto tiempo tarda en procesarse una solicitud y c贸mo ha cambiado esta velocidad.

**Patrones estacionales:** Identificar tendencias estacionales en la presentaci贸n de solicitudes, como picos en ciertos meses del a帽o.



### ***Proceso de creaci贸n del pipeline***

- Verificaci贸n del [c贸digo](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/first_code_python) y acceso a la API: 
  Antes de crear el DAG, verifiqu茅 el funcionamiento del c贸digo y la correcta conexi贸n a la API
  para asegurarme de que los datos se obtuvieran correctamente.

RESULTADO OBTENIDO:

![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/first_code.PNG)



- Creaci贸n del [DAG:](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/DAG_python) Basado en el c贸digo verificado, cre茅 un DAG en Airflow para automatizar el proceso de carga 
  de datos.
  Guardado los datos en AWS S3: Una vez que los datos fueron guardados en AWS S3, verifiqu茅 su disponibilidad y la integridad.


  
  RESUNTADO OBTENIDO:


  
  
  ![](https://github.com/user-attachments/assets/b9b02c38-67f6-40db-92b5-828767957273)




  ![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/s3_dag.PNG)









  

  

  



- Acceso a los datos en S3: Despu茅s de guardar los datos en S3, utilic茅 biblioteca Dask para analizarlos y extraerlos desde S3.
  [Este c贸digo](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/result_code_python) es una demostraci贸n y verificaci贸n de que 
  nuestro DAG funciona correctamente, guardando y leyendo datos desde AWS S3 de manera exitosa. 
  
   ***El codigo realiza las siguientes acciones:***

   1. Crea una sesi贸n y cliente S3 utilizando las credenciales de AWS.

   2. Verifica si un archivo espec铆fico est谩 disponible en un bucket S3.

   3. Lee el archivo Parquet desde S3 utilizando Dask.

   4. Muestra las primeras filas del DataFrame.

   5. Calcula y muestra el n煤mero de filas y columnas.

   6. Encuentra y cuenta los duplicados en el DataFrame.

  **Ahora, con los datos disponibles, es posible trabajar con ellos y realizar diferentes an谩lisis anal铆ticos.**

    ![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/final_code.PNG)
  




