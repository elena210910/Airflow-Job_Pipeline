# Airflow-Job_Pipeline

![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/apache%20air.jpg)


## Objetivo del Proyecto
El objetivo es crear un pipeline que cargue los datos autom√°ticamente una vez al d√≠a utilizando Apache Airflow.üöÄ 
Esto permitir√° mantener los datos actualizados y disponibles para an√°lisis continuos sin intervenci√≥n manual.
Adem√°s, los datos finales ser√°n cargados en AWS S3, asegurando que est√©n accesibles y almacenados de manera segura.


## Sobre los datos con los que vamos a trabajar

Descripci√≥n de los datos y su utilidad para el estudio
Este conjunto de datos contiene todas las solicitudes de trabajo presentadas a trav√©s de las oficinas de los distritos,
mediante eFiling o a trav√©s de HUB, que tienen una "Fecha de √∫ltima acci√≥n" desde el 1 de enero de 2000. Este conjunto de datos no incluye trabajos presentados a trav√©s de DOB NOW.
[El enlace de la p√°gina web que proporciona los datos.](https://data.cityofnewyork.us/Housing-Development/DOB-Job-Application-Filings/ic3t-wcy2/about_data)

## Utilidad del estudio de estos datos

**Tendencias hist√≥ricas:** Analizar las solicitudes de trabajo a lo largo del tiempo puede 
mostrar c√≥mo han cambiado las tendencias en la industria de la construcci√≥n.

**Diferencias regionales:** Identificar c√≥mo se distribuyen las solicitudes en diferentes distritos y oficinas.

**Tipos de proyectos:** Comparar las solicitudes por diferentes tipos de proyectos para identificar patrones predominantes.

**Velocidad de procesamiento de solicitudes:** Evaluar cu√°nto tiempo tarda en procesarse una solicitud y c√≥mo ha cambiado esta velocidad.

**Patrones estacionales:** Identificar tendencias estacionales en la presentaci√≥n de solicitudes, como picos en ciertos meses del a√±o.




### ***Proceso de creaci√≥n del pipeline***

- Verificaci√≥n del [c√≥digo](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/first_code_python) y acceso a la API: 
  Antes de crear el DAG, verifiqu√© el funcionamiento del c√≥digo y la correcta conexi√≥n a la API
  para asegurarme de que los datos se obtuvieran correctamente.
  Los datos obtenidos a trav√©s del API vienen en formato JSON, pero como tienen una estructura organizada (cada objeto tiene las mismas claves),
  se pueden transformar f√°cilmente en un formato tabular utilizando pandas y un DataFrame.
  Esto me permite manejar y analizar los datos de manera m√°s eficiente en un entorno de an√°lisis estructurado.


RESULTADO OBTENIDO:

![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/first_code.PNG)



- Creaci√≥n del [DAG:](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/DAG_python) Basado en el c√≥digo verificado, cre√© un DAG en Airflow para automatizar el proceso de carga 
  de datos.
  Guardado los datos en AWS S3: Una vez que los datos fueron guardados en AWS S3, verifiqu√© su disponibilidad y la integridad.


  
  RESUNTADO OBTENIDO:


  
  
  ![](https://github.com/user-attachments/assets/b9b02c38-67f6-40db-92b5-828767957273)




  ![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/s3_dag.PNG)









  

  

  



- Acceso a los datos en S3: Despu√©s de guardar los datos en S3, utilic√© biblioteca Dask para analizarlos y extraerlos desde S3.
  [Este c√≥digo](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/result_code_python) es una demostraci√≥n y verificaci√≥n de que 
  nuestro DAG funciona correctamente, guardando y leyendo datos desde AWS S3 de manera exitosa. üéØ
  
   ***El codigo realiza las siguientes acciones:***

   ‚úÖ Crea una sesi√≥n y cliente S3 utilizando las credenciales de AWS.

   ‚úÖ Verifica si un archivo espec√≠fico est√° disponible en un bucket S3.

   ‚úÖ Lee el archivo Parquet desde S3 utilizando Dask.

   ‚úÖ Muestra las primeras filas del DataFrame.

   ‚úÖ Calcula y muestra el n√∫mero de filas y columnas.

   ‚úÖ Encuentra y cuenta los duplicados en el DataFrame.

  **Ahora, con los datos disponibles, es posible trabajar con ellos y realizar diferentes an√°lisis anal√≠ticos.**

    ![](https://github.com/elena210910/Airflow-Job_Pipeline/blob/main/final_code.PNG)
  




